"""
æ™ºèƒ½æ–‡ä»¶åˆ†æå·¥å…·

ğŸ”¥ æ ¸å¿ƒè®¾è®¡ï¼ˆè§ docs/CONTEXT.mdï¼‰ï¼š
- analyze_and_classifyï¼šä¸€é”®åˆ†æ+åˆ†ç±»ï¼Œè¾“å‡ºç»“æœä¾›ç”¨æˆ·ç¡®è®¤/ä¿®æ­£

æ•°æ®æµï¼š
1. scan_media_files() â†’ scanned_files (List[ScannedFile])
2. analyze_and_classify() â†’ åˆ†æ + åˆ†ç±» + è¾“å‡ºç»“æœä¾›ç”¨æˆ·ç¡®è®¤
3. ç”¨æˆ·ç¡®è®¤ â†’ generate_strm / organize_files

ğŸ”¥ æ–°æ¶æ„ï¼ˆ2026-01-09ï¼‰ï¼š
- ä½¿ç”¨ InjectedState è®¿é—® State
- è¿”å›é€šç”¨ ToolResponse JSONï¼š{"message": "...", "state_update": {...}}
- classifications é€šè¿‡ state_update è¿”å›

ğŸ”¥ åˆ†ç±»é€»è¾‘é‡æ„ï¼ˆ2026-01-09ï¼‰ï¼š
- ä»£ç ä¸åˆ¤æ–­ï¼ŒåªæŸ¥è¡¨
- ä½¿ç”¨ TMDBMapping æ˜ å°„è¡¨
- context ç”± LLM åˆ†æç”Ÿæˆ
"""

import re
import json
import logging
from typing import Dict, Any, List, Tuple, Annotated
from collections import defaultdict
from langchain.tools import tool
from langgraph.prebuilt import InjectedState

from backend.agents.state import MediaAgentState
from backend.agents.tool_response import make_tool_response
from backend.services.tmdb_service import get_tmdb_service
from backend.agents.models import (
    ScannedFile,
    Classification,
    ClassifiedFile,
    SubtitleFile,
    MediaType,
    SubCategory,
    determine_subcategory,
    ClassificationResultItem,
    # æ–°æ¶æ„ï¼šæ˜ å°„è¡¨
    TMDBMapping,
    get_or_build_mapping,
)
from backend.agents.models.output import SeasonInfo
from backend.agents.classifier import (
    classify_file,
    classify_files,
    summarize_results,
    ClassifyResult,
    extract_episode_number as new_extract_episode_number,
)

logger = logging.getLogger(__name__)


# ============ è¾…åŠ©å‡½æ•° ============

def _extract_episode_number(filename: str) -> int:
    """ä»æ–‡ä»¶åæå–é›†æ•°
    
    æ³¨æ„ï¼šéœ€è¦æ’é™¤å¸¸è§çš„ç¼–ç ä¿¡æ¯å¦‚ x264, x265, h264, h265
    """
    # å…ˆç§»é™¤å¯èƒ½å¹²æ‰°çš„ç¼–ç ä¿¡æ¯
    clean_name = re.sub(r'[xh]26[45]', '', filename, flags=re.IGNORECASE)
    clean_name = re.sub(r'HEVC|AVC|Ma10p|10bit', '', clean_name, flags=re.IGNORECASE)
    
    patterns = [
        r'EP?\.?(\d{2,4})',           # EP01, E01, EP.01
        r'(?<![xh])E(\d{2,4})',       # E01 ä½†ä¸åŒ¹é… x265
        r'ç¬¬(\d{1,4})[é›†è¯è©±]',        # ç¬¬01é›†
        r'\[(\d{2,4})\]',             # [01]
        r'[\.\s\-_](\d{2,4})[\.\s\-_\[]',  # .01. _01_
        r'S\d+E(\d{2,4})',            # S01E01
    ]
    
    for pattern in patterns:
        match = re.search(pattern, clean_name, re.IGNORECASE)
        if match:
            ep = int(match.group(1))
            # æ’é™¤ä¸åˆç†çš„é›†æ•°ï¼ˆå¦‚ 1080, 720 ç­‰åˆ†è¾¨ç‡ï¼‰
            if ep < 1000 and ep > 0:
                return ep
    
    return 0


def _get_base_name(filename: str) -> str:
    """æå–æ–‡ä»¶ä¸»åç§°ï¼ˆå»æ‰è¯­è¨€æ ‡è¯†å’Œæ‰©å±•åï¼‰
    
    Examples:
        [001].chs.srt â†’ [001]
        [001].mkv â†’ [001]
        Movie.2020.chs.ass â†’ Movie.2020
        [001].scjp.ass â†’ [001]
        [001].tcjp.ass â†’ [001]
    """
    # ç§»é™¤æ‰©å±•å
    name = re.sub(r'\.(srt|ass|ssa|sub|mkv|mp4|avi|wmv|flv|mov)$', '', filename, flags=re.IGNORECASE)
    # ç§»é™¤è¯­è¨€æ ‡è¯†ï¼ˆåŒ…æ‹¬å¤åˆè¯­è¨€æ ‡è¯†å¦‚ scjp, tcjpï¼‰
    name = re.sub(r'\.(chs|cht|chi|eng|jpn|jap|kor|und|sc|tc|scjp|tcjp|chtjp|chsjp)$', '', name, flags=re.IGNORECASE)
    return name


def _parse_scanned_files(scanned_files_data: List[Dict[str, Any]]) -> List[ScannedFile]:
    """ä» State ä¸­è§£æ scanned_files æ•°æ®ä¸º Pydantic æ¨¡å‹
    
    ğŸ”¥ ä½¿ç”¨ Pydantic model_validate è‡ªåŠ¨éªŒè¯å’Œè½¬æ¢
    """
    return [ScannedFile.model_validate(f) for f in scanned_files_data]


def _execute_classification(
    mappings: List[dict],
    video_files: List[ScannedFile],
    subtitle_files: List[ScannedFile]
) -> Tuple[Dict[int, Classification], List[ScannedFile], int]:
    """
    æ‰§è¡Œåˆ†ç±»é€»è¾‘
    
    Returns:
        (classifications, unclassified_files, matched_count)
    """
    tmdb = get_tmdb_service()
    
    # æ„å»ºå­—å¹•ç´¢å¼•
    subtitle_index = defaultdict(list)
    for sub in subtitle_files:
        base_name = _get_base_name(sub.name)
        subtitle_index[(sub.directory, base_name)].append(sub)
    
    # åˆå§‹åŒ–åˆ†ç±»ç»“æ„
    classifications: Dict[int, Classification] = {}
    tmdb_seasons_cache: Dict[int, List[Dict]] = {}
    
    # æ”¶é›†æ‰€æœ‰ TMDB ID
    tv_ids = set()
    movie_ids = set()
    
    for mapping in mappings:
        tmdb_id = mapping.get('tmdb_id')
        if not tmdb_id or tmdb_id == 0:
            continue
            
        if mapping.get('type') == 'tv':
            tv_ids.add(tmdb_id)
        elif mapping.get('type') == 'movie':
            movie_ids.add(tmdb_id)
            for fm in mapping.get('file_mappings', []):
                if fm.get('tmdb_id'):
                    movie_ids.add(fm['tmdb_id'])
    
    # è·å– TV è¯¦æƒ…
    for tmdb_id in tv_ids:
        tmdb_info = tmdb.get_tv_details(tmdb_id)
        if tmdb_info:
            genres = tmdb_info.genres if tmdb_info.genres else []
            sub_category = determine_subcategory(genres)
            name = tmdb_info.title_zh or tmdb_info.title or f"TMDB:{tmdb_id}"
            
            classifications[tmdb_id] = Classification(
                tmdb_id=tmdb_id,
                name=name,
                type=MediaType.TV,
                year=tmdb_info.year,
                genres=genres,
                sub_category=sub_category,
                seasons={},
                files=[]
            )
            
            seasons = tmdb.get_tv_all_seasons(tmdb_id)
            tmdb_seasons_cache[tmdb_id] = seasons
    
    # è·å– Movie è¯¦æƒ…
    for tmdb_id in movie_ids:
        tmdb_info = tmdb.get_movie_details(tmdb_id)
        if tmdb_info:
            genres = tmdb_info.genres if tmdb_info.genres else []
            sub_category = determine_subcategory(genres)
            name = tmdb_info.title_zh or tmdb_info.title or f"TMDB:{tmdb_id}"
            
            classifications[tmdb_id] = Classification(
                tmdb_id=tmdb_id,
                name=name,
                type=MediaType.MOVIE,
                year=tmdb_info.year,
                genres=genres,
                sub_category=sub_category,
                seasons={},
                files=[]
            )
    
    def _find_season_for_episode(tmdb_id: int, ep: int, use_global: bool = True) -> tuple:
        """æ ¹æ®é›†æ•°ç¡®å®šå­£å·å’Œ TMDB episode_number"""
        seasons = tmdb_seasons_cache.get(tmdb_id, [])
        
        for s in seasons:
            if use_global:
                if s['ep_start_global'] <= ep <= s['ep_end_global']:
                    pos = ep - s['ep_start_global']
                    tmdb_ep = s['ep_start'] + pos
                    return s['season_number'], tmdb_ep
            else:
                if s['ep_start'] <= ep <= s['ep_end']:
                    return s['season_number'], ep
        
        return None, ep
    
    matched_count = 0
    
    # å¤„ç†æ¯ä¸ª mapping
    for mapping in mappings:
        path_pattern = mapping.get('path', '')
        mapping_type = mapping.get('type', 'tv')
        
        # æ‰¾å‡ºåŒ¹é…æ­¤è·¯å¾„çš„æ–‡ä»¶
        matching_files = []
        for vf in video_files:
            file_dir = vf.directory
            if path_pattern.lower() in file_dir.lower() or path_pattern.lower() in vf.path.lower():
                matching_files.append(vf)
        
        if mapping_type == 'movie':
            tmdb_id = mapping.get('tmdb_id')
            file_mappings = mapping.get('file_mappings', [])
            
            if file_mappings:
                for fm in file_mappings:
                    pattern = fm.get('pattern', '').lower()
                    fm_tmdb_id = fm.get('tmdb_id')
                    
                    if not fm_tmdb_id or fm_tmdb_id not in classifications:
                        continue
                    
                    for vf in matching_files:
                        if pattern in vf.name.lower():
                            base_name = _get_base_name(vf.name)
                            subs = subtitle_index.get((vf.directory, base_name), [])
                            
                            classified_file = ClassifiedFile(
                                path=vf.path,
                                name=vf.name,
                                episode=0,
                                season=0,
                                subtitles=[
                                    SubtitleFile(path=s.path, name=s.name, language=s.language or "und")
                                    for s in subs
                                ]
                            )
                            classifications[fm_tmdb_id].files.append(classified_file)
                            matched_count += 1
            elif tmdb_id and tmdb_id in classifications:
                for vf in matching_files:
                    base_name = _get_base_name(vf.name)
                    subs = subtitle_index.get((vf.directory, base_name), [])
                    
                    classified_file = ClassifiedFile(
                        path=vf.path,
                        name=vf.name,
                        episode=0,
                        season=0,
                        subtitles=[
                            SubtitleFile(path=s.path, name=s.name, language=s.language or "und")
                            for s in subs
                        ]
                    )
                    classifications[tmdb_id].files.append(classified_file)
                    matched_count += 1
        
        elif mapping_type == 'tv':
            tmdb_id = mapping.get('tmdb_id')
            if not tmdb_id or tmdb_id not in classifications:
                continue
            
            season = mapping.get('season')
            episode_range = mapping.get('episode_range')
            offset = mapping.get('offset', 0)
            numbering = mapping.get('numbering', 'direct')
            
            for vf in matching_files:
                ep = _extract_episode_number(vf.name)
                if ep == 0:
                    continue
                
                if episode_range:
                    ep_start, ep_end = episode_range
                    if not (ep_start <= ep <= ep_end):
                        continue
                
                if season is not None:
                    season_num = season
                    if numbering == 'direct':
                        tmdb_ep = ep
                    elif numbering == 'offset':
                        tmdb_ep = ep + offset
                    elif numbering == 'global_to_season':
                        _, tmdb_ep = _find_season_for_episode(tmdb_id, ep, use_global=True)
                    else:
                        tmdb_ep = ep
                else:
                    adjusted_ep = ep + offset
                    use_global = numbering in ['direct', 'global_to_season']
                    season_num, tmdb_ep = _find_season_for_episode(tmdb_id, adjusted_ep, use_global=use_global)
                    
                    if season_num is None:
                        logger.warning(f"EP{ep} æœªæ‰¾åˆ°å¯¹åº”å­£ (TMDB:{tmdb_id})")
                        continue
                
                if season_num not in classifications[tmdb_id].seasons:
                    classifications[tmdb_id].seasons[season_num] = []
                
                base_name = _get_base_name(vf.name)
                subs = subtitle_index.get((vf.directory, base_name), [])
                
                classified_file = ClassifiedFile(
                    path=vf.path,
                    name=vf.name,
                    episode=tmdb_ep,
                    season=season_num,
                    subtitles=[
                        SubtitleFile(path=s.path, name=s.name, language=s.language or "und")
                        for s in subs
                    ]
                )
                classifications[tmdb_id].seasons[season_num].append(classified_file)
                matched_count += 1
    
    # æ‰¾å‡ºæœªåˆ†ç±»çš„æ–‡ä»¶
    classified_paths = set()
    for cls in classifications.values():
        for files in cls.seasons.values():
            for f in files:
                classified_paths.add(f.path)
        for f in cls.files:
            classified_paths.add(f.path)
    
    unclassified = [f for f in video_files if f.path not in classified_paths]
    
    return classifications, unclassified, matched_count


def _classifications_to_list(classifications: Dict[int, Classification]) -> List[Dict[str, Any]]:
    """å°† classifications è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„åˆ—è¡¨æ ¼å¼
    
    ğŸ”¥ ä½¿ç”¨ Pydantic model_dump è‡ªåŠ¨é€’å½’åºåˆ—åŒ–åµŒå¥—å¯¹è±¡
    """
    return [cls.model_dump() for cls in classifications.values()]


# ============ å·¥å…·å‡½æ•° ============

@tool
def analyze_and_classify(
    mappings_json: str,
    state: Annotated[dict, InjectedState] = None,
) -> str:
    """
    ğŸ”¥ æ‰§è¡Œæ–‡ä»¶åˆ†ç±»ï¼ˆæ ¹æ® LLM çš„ mappingsï¼‰
    
    æ ¹æ® LLM åˆ†æåç”Ÿæˆçš„ mappingsï¼Œå°†æ–‡ä»¶åˆ†ç±»åˆ°æ­£ç¡®çš„ TMDB æ¡ç›®å’Œå­£ã€‚
    
    Args:
        mappings_json: LLM ç”Ÿæˆçš„ mappings JSONï¼Œæ ¼å¼ï¼š
            {
              "mappings": [
                // ç±»å‹ 1ï¼šæŒ‰ç›®å½•/å­£
                {"path": "<ç›®å½•å>", "tmdb_id": <TVç³»åˆ—ID>, "type": "tv", "season": 1},
                
                // ç±»å‹ 2ï¼šæŒ‰é›†æ•°èŒƒå›´/ç³»åˆ—ï¼ˆæ··åˆç›®å½•ï¼‰
                {"path": "<ç›®å½•å>", "episode_range": [1, 220], "tmdb_id": <ç³»åˆ—A_ID>, "type": "tv"},
                {"path": "<ç›®å½•å>", "episode_range": [221, 720], "tmdb_id": <ç»­ä½œID>, "type": "tv", "offset": -220},
                
                // ç±»å‹ 3ï¼šæŒ‰æ–‡ä»¶å/ç”µå½±
                {"path": "ç”µå½±/", "type": "movie", "file_mappings": [
                    {"pattern": "Iron Man", "tmdb_id": 1726}
                ]}
              ]
            }
    
    Returns:
        ToolResponse JSONï¼ŒåŒ…å«åˆ†ç±»ç»“æœ
    
    ä½¿ç”¨æµç¨‹ï¼ˆLLM é©±åŠ¨ï¼‰ï¼š
    1. scan_media_files() â†’ æ‰«ææ–‡ä»¶
    2. LLM åˆ†ææ–‡ä»¶å/ç›®å½•ç»“æ„ â†’ ç”Ÿæˆ mappings JSON
    3. analyze_and_classify(mappings_json) â†’ æ‰§è¡Œåˆ†ç±»å¹¶è¿”å›ç»“æœ
    4. ç”¨æˆ·ç¡®è®¤ â†’ generate_strm æˆ– organize_files
    """
    # ğŸ”¥ é¦–å…ˆè®°å½•æ”¶åˆ°çš„å‚æ•° - ä½¿ç”¨ print ç¡®ä¿è¾“å‡º
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ analyze_and_classify è¢«è°ƒç”¨")
    logger.info(f"ğŸ“Š analyze_and_classify è¢«è°ƒç”¨")
    logger.info(f"ğŸ“Š mappings_json ç±»å‹: {type(mappings_json)}, é•¿åº¦: {len(mappings_json) if mappings_json else 0}")
    logger.info(f"ğŸ“Š mappings_json å†…å®¹: {mappings_json[:500] if mappings_json else 'None'}...")
    logger.info(f"ğŸ“Š state ç±»å‹: {type(state)}, æ˜¯å¦ä¸ºç©º: {state is None}")
    
    # ä» State è¯»å–æ•°æ®
    scanned_files_data = state.get("scanned_files", []) if state else []
    logger.info(f"ğŸ“Š scanned_files_data é•¿åº¦: {len(scanned_files_data)}")
    
    if not scanned_files_data:
        logger.warning("âŒ scanned_files_data ä¸ºç©º")
        return make_tool_response("âŒ è¯·å…ˆä½¿ç”¨ scan_media_files æ‰«ææ–‡ä»¶")
    
    # è§£æ scanned_files
    scanned_files = _parse_scanned_files(scanned_files_data)
    
    # åˆ†ç¦»è§†é¢‘å’Œå­—å¹•æ–‡ä»¶
    video_files = [f for f in scanned_files if f.type == 'video']
    subtitle_files = [f for f in scanned_files if f.type == 'subtitle']
    
    if not video_files:
        return make_tool_response("âŒ æ‰«æç»“æœä¸­æ²¡æœ‰è§†é¢‘æ–‡ä»¶")
    
    # ============ 1. è§£æ mappings ============
    
    logger.info(f"ğŸ“Š analyze_and_classify æ”¶åˆ° mappings_json: {mappings_json[:200] if mappings_json else 'None'}...")
    
    if not mappings_json or not mappings_json.strip():
        logger.warning("âŒ mappings_json ä¸ºç©º")
        return make_tool_response("âŒ è¯·æä¾› mappings_json å‚æ•°")
    
    mappings_json = mappings_json.strip()
    mappings = []
    
    try:
        if mappings_json.startswith('{') or mappings_json.startswith('['):
            parsed = json.loads(mappings_json)
            if isinstance(parsed, dict) and 'mappings' in parsed:
                mappings = parsed['mappings']
            elif isinstance(parsed, list):
                mappings = parsed
            logger.info(f"ğŸ“Š è§£æå¾—åˆ° {len(mappings)} ä¸ª mappings")
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return make_tool_response(f"âŒ JSON è§£æå¤±è´¥: {e}\nè¯·æ£€æŸ¥ mappings æ ¼å¼")
    
    if not mappings:
        logger.warning("âŒ mappings ä¸ºç©ºåˆ—è¡¨")
        return make_tool_response("âŒ mappings ä¸ºç©ºï¼Œè¯·æä¾›æœ‰æ•ˆçš„åˆ†ç±»é…ç½®")
    
    # ============ 2. æ‰§è¡Œåˆ†ç±» ============
    
    classifications, unclassified, matched_count = _execute_classification(
        mappings, video_files, subtitle_files
    )
    
    # ============ 3. ç”Ÿæˆåˆ†ç±»ç»“æœæŠ¥å‘Š ============
    
    output = "# ğŸ“Š åˆ†ç±»ç»“æœ\n\n"
    output += f"**å·²åˆ†ç±»**: {matched_count} / {len(video_files)} ä¸ªæ–‡ä»¶\n\n"
    
    # åˆ†ç±»ç»“æœ
    for tmdb_id, cls in classifications.items():
        if cls.type == MediaType.TV:
            output += f"### ğŸ“º {cls.name} (TMDB:{tmdb_id})\n\n"
            output += "| å­£ | æ–‡ä»¶æ•° | é›†æ•°èŒƒå›´ |\n"
            output += "|---|--------|----------|\n"
            
            total_files = 0
            for season_num in sorted(cls.seasons.keys()):
                files = cls.seasons[season_num]
                if files:
                    eps = sorted([f.episode for f in files if f.episode > 0])
                    if eps:
                        output += f"| S{season_num:02d} | {len(files)} | E{eps[0]:02d}-E{eps[-1]:02d} |\n"
                    else:
                        output += f"| S{season_num:02d} | {len(files)} | - |\n"
                    total_files += len(files)
            
            output += f"\n**å°è®¡: {total_files} ä¸ªæ–‡ä»¶**\n\n"
        
        else:  # movie
            output += f"### ğŸ¬ {cls.name} (TMDB:{tmdb_id})\n\n"
            output += f"**æ–‡ä»¶æ•°**: {len(cls.files)} ä¸ª\n\n"
    
    # æœªåˆ†ç±»æ–‡ä»¶
    if unclassified:
        output += f"## âš ï¸ æœªåˆ†ç±»æ–‡ä»¶: {len(unclassified)} ä¸ª\n\n"
        for f in unclassified[:10]:
            ep = _extract_episode_number(f.name)
            ep_str = f"EP{ep:03d}" if ep else "-"
            output += f"- {f.name} ({ep_str})\n"
        if len(unclassified) > 10:
            output += f"- ... è¿˜æœ‰ {len(unclassified) - 10} ä¸ª\n"
        output += "\n"
    
    # ä¸‹ä¸€æ­¥æç¤º
    output += "---\n\n"
    output += "## ğŸ¯ ä¸‹ä¸€æ­¥\n\n"
    if unclassified:
        output += "âš ï¸ æœ‰æœªåˆ†ç±»æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦ä¿®æ­£ mappings åé‡æ–°åˆ†ç±»ã€‚\n\n"
    output += "è¯·é€‰æ‹©æ“ä½œï¼š\n"
    output += "- **æ‰§è¡Œ STRM**: `connect_strm_target` â†’ `generate_strm`\n"
    output += "- **æ‰§è¡Œä¼ ç»Ÿæ•´ç†**: `organize_files`\n"
    output += "- **é‡æ–°åˆ†ç±»**: ä¿®æ­£ mappings åå†æ¬¡è°ƒç”¨ `analyze_and_classify`\n"
    
    logger.info(f"ğŸ“Š analyze_and_classify è¾“å‡º: {len(output)} å­—ç¬¦")
    
    # è½¬æ¢ classifications ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    classifications_list = _classifications_to_list(classifications)
    
    # æ„å»ºå‰ç«¯æœŸæœ›çš„ classification_result æ ¼å¼ï¼ˆæŒ‰å­£æ˜¾ç¤ºï¼‰
    # ğŸ”¥ ä½¿ç”¨ Pydantic ClassificationResultItem éªŒè¯è¾“å‡ºæ ¼å¼
    classification_result = {}
    for tmdb_id, cls in classifications.items():
        if cls.type == MediaType.TV:
            # è®¡ç®—æ€»æ–‡ä»¶æ•°
            total_files = sum(len(files) for files in cls.seasons.values())
            
            # ğŸ”¥ æŒ‰å­£æ„å»ºè¯¦æƒ…
            seasons_info = []
            all_eps = []
            for season_num, files in sorted(cls.seasons.items()):
                eps = [f.episode for f in files if f.episode > 0]
                if eps:
                    all_eps.extend(eps)
                    season_ep_range = f"E{min(eps):02d}-E{max(eps):02d}"
                    seasons_info.append(SeasonInfo(
                        season=season_num,
                        episode_count=len(eps),
                        ep_range=season_ep_range
                    ))
            
            # å…¼å®¹æ—§ç‰ˆçš„ ep_range
            ep_range = f"E{min(all_eps):02d}-E{max(all_eps):02d}" if all_eps else "-"
            
            # ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯
            item = ClassificationResultItem(
                name=cls.name,
                file_count=total_files,
                ep_range=ep_range,
                type="tv",
                seasons=[s.model_dump() for s in seasons_info]  # ğŸ†• æŒ‰å­£è¯¦æƒ…
            )
            classification_result[str(tmdb_id)] = item.model_dump()
        else:  # movie
            item = ClassificationResultItem(
                name=cls.name,
                file_count=len(cls.files),
                ep_range="-",
                type="movie",
                seasons=[]  # ç”µå½±æ²¡æœ‰å­£
            )
            classification_result[str(tmdb_id)] = item.model_dump()
    
    # è¿”å› ToolResponse JSONï¼ˆåŒ…å« classificationsï¼‰
    return make_tool_response(
        output,
        state_update={
            "classifications": classifications_list,
            "classification_result": classification_result,
        }
    )


@tool
def analyze_and_classify_v2(
    mappings_json: str,
    state: Annotated[dict, InjectedState] = None,
) -> str:
    """
    ğŸ”¥ æ‰§è¡Œæ–‡ä»¶åˆ†ç±» V2ï¼ˆæ–°æ¶æ„ï¼šä»£ç ä¸åˆ¤æ–­ï¼ŒåªæŸ¥è¡¨ï¼‰
    
    æ ¹æ® LLM åˆ†æåç”Ÿæˆçš„ mappingsï¼Œå°†æ–‡ä»¶åˆ†ç±»åˆ°æ­£ç¡®çš„ TMDB æ¡ç›®å’Œå­£ã€‚
    
    Args:
        mappings_json: LLM ç”Ÿæˆçš„ mappings JSONï¼Œæ ¼å¼ï¼š
            {
              "mappings": [
                // TVï¼šæŒ‡å®š context
                {"path_pattern": "ç¬¬ä¸€å­£", "tmdb_id": 30977, "context": "season_1"},
                {"path_pattern": "ç¬¬äºŒå­£", "tmdb_id": 30977, "context": "season_2"},
                {"path_pattern": "å…¨é›†", "tmdb_id": 46260, "context": "cumulative"},
                
                // Movie
                {"file_pattern": "é’¢é“ä¾ ", "tmdb_id": 1726, "media_type": "movie"}
              ]
            }
            
            context è¯´æ˜ï¼š
            - "cumulative": æ–‡ä»¶ç¼–å·æ˜¯å…¨ç³»åˆ—ç´¯è®¡ç¼–å·
            - "season_N": æ–‡ä»¶ç¼–å·æ˜¯ç¬¬ N å­£çš„å­£å†…ç¼–å·
    
    Returns:
        ToolResponse JSONï¼ŒåŒ…å«åˆ†ç±»ç»“æœ
    """
    logger.info(f"ğŸ”¥ analyze_and_classify_v2 è¢«è°ƒç”¨")
    
    # ä» State è¯»å–æ•°æ®
    scanned_files_data = state.get("scanned_files", []) if state else []
    
    if not scanned_files_data:
        return make_tool_response("âŒ è¯·å…ˆä½¿ç”¨ scan_media_files æ‰«ææ–‡ä»¶")
    
    # è§£æ scanned_files
    scanned_files = _parse_scanned_files(scanned_files_data)
    
    # åˆ†ç¦»è§†é¢‘å’Œå­—å¹•æ–‡ä»¶
    video_files = [f for f in scanned_files if f.type == 'video']
    subtitle_files = [f for f in scanned_files if f.type == 'subtitle']
    
    if not video_files:
        return make_tool_response("âŒ æ‰«æç»“æœä¸­æ²¡æœ‰è§†é¢‘æ–‡ä»¶")
    
    # è§£æ mappings
    if not mappings_json or not mappings_json.strip():
        return make_tool_response("âŒ è¯·æä¾› mappings_json å‚æ•°")
    
    mappings_json = mappings_json.strip()
    mappings = []
    
    try:
        if mappings_json.startswith('{') or mappings_json.startswith('['):
            parsed = json.loads(mappings_json)
            if isinstance(parsed, dict) and 'mappings' in parsed:
                mappings = parsed['mappings']
            elif isinstance(parsed, list):
                mappings = parsed
    except json.JSONDecodeError as e:
        return make_tool_response(f"âŒ JSON è§£æå¤±è´¥: {e}\nè¯·æ£€æŸ¥ mappings æ ¼å¼")
    
    if not mappings:
        return make_tool_response("âŒ mappings ä¸ºç©ºï¼Œè¯·æä¾›æœ‰æ•ˆçš„åˆ†ç±»é…ç½®")
    
    # ============ æ„å»º TMDB æ˜ å°„è¡¨ ============
    
    tmdb = get_tmdb_service()
    tmdb_mappings: Dict[int, TMDBMapping] = {}
    tmdb_info_cache: Dict[int, Any] = {}
    
    # æ”¶é›†æ‰€æœ‰ TMDB ID
    for m in mappings:
        tmdb_id = m.get("tmdb_id", 0)
        media_type = m.get("media_type", "tv")
        
        if tmdb_id and tmdb_id not in tmdb_mappings and media_type == "tv":
            mapping = get_or_build_mapping(tmdb_id, tmdb)
            if mapping:
                tmdb_mappings[tmdb_id] = mapping
                tmdb_info_cache[tmdb_id] = {
                    "name": mapping.title,
                    "type": "tv",
                }
        
        if tmdb_id and media_type == "movie":
            movie_info = tmdb.get_movie_details(tmdb_id)
            if movie_info:
                tmdb_info_cache[tmdb_id] = {
                    "name": movie_info.title_zh or movie_info.title or f"TMDB:{tmdb_id}",
                    "type": "movie",
                }
    
    # ============ æ‰§è¡Œåˆ†ç±»ï¼ˆåªæŸ¥è¡¨ï¼‰ ============
    
    files_for_classify = [
        {
            "path": f.path,
            "name": f.name,
            "directory": f.directory,
        }
        for f in video_files
    ]
    
    results = classify_files(files_for_classify, mappings, tmdb_mappings)
    summary = summarize_results(results)
    
    # ============ æ„å»ºå­—å¹•ç´¢å¼• ============
    
    subtitle_index = defaultdict(list)
    for sub in subtitle_files:
        base_name = _get_base_name(sub.name)
        subtitle_index[(sub.directory, base_name)].append(sub)
    
    # ============ è½¬æ¢ä¸ºæ—§æ ¼å¼ï¼ˆå…¼å®¹åç»­å·¥å…·ï¼‰ ============
    
    classifications: Dict[int, Classification] = {}
    
    for r in results:
        if r.status != "matched":
            continue
        
        tmdb_id = r.tmdb_id
        if tmdb_id not in classifications:
            info = tmdb_info_cache.get(tmdb_id, {})
            tmdb_info = tmdb.get_tv_details(tmdb_id) if info.get("type") == "tv" else tmdb.get_movie_details(tmdb_id)
            genres = tmdb_info.genres if tmdb_info else []
            sub_category = determine_subcategory(genres)
            
            classifications[tmdb_id] = Classification(
                tmdb_id=tmdb_id,
                name=info.get("name", f"TMDB:{tmdb_id}"),
                type=MediaType.TV if info.get("type") == "tv" else MediaType.MOVIE,
                year=tmdb_info.year if tmdb_info else 0,
                genres=genres,
                sub_category=sub_category,
                seasons={},
                files=[]
            )
        
        # æ·»åŠ å­—å¹•
        base_name = _get_base_name(r.file_name)
        file_dir = "/".join(r.file_path.rsplit("/", 1)[:-1]) if "/" in r.file_path else ""
        subs = subtitle_index.get((file_dir, base_name), [])
        
        classified_file = ClassifiedFile(
            path=r.file_path,
            name=r.file_name,
            episode=r.episode,
            season=r.season,
            subtitles=[
                SubtitleFile(path=s.path, name=s.name, language=s.language or "und")
                for s in subs
            ]
        )
        
        if r.season > 0:
            if r.season not in classifications[tmdb_id].seasons:
                classifications[tmdb_id].seasons[r.season] = []
            classifications[tmdb_id].seasons[r.season].append(classified_file)
        else:
            classifications[tmdb_id].files.append(classified_file)
    
    # ============ ç”ŸæˆæŠ¥å‘Š ============
    
    output = "# ğŸ“Š åˆ†ç±»ç»“æœ (V2 æ–°æ¶æ„)\n\n"
    output += f"**å·²åˆ†ç±»**: {summary['matched']} / {summary['total']} ä¸ªæ–‡ä»¶\n\n"
    
    # åˆ†ç±»ç»“æœ
    for tmdb_id, cls in classifications.items():
        if cls.type == MediaType.TV:
            output += f"### ğŸ“º {cls.name} (TMDB:{tmdb_id})\n\n"
            output += "| å­£ | æ–‡ä»¶æ•° | é›†æ•°èŒƒå›´ |\n"
            output += "|---|--------|----------|\n"
            
            total_files = 0
            for season_num in sorted(cls.seasons.keys()):
                files = cls.seasons[season_num]
                if files:
                    eps = sorted([f.episode for f in files if f.episode > 0])
                    if eps:
                        output += f"| S{season_num:02d} | {len(files)} | E{eps[0]:02d}-E{eps[-1]:02d} |\n"
                    else:
                        output += f"| S{season_num:02d} | {len(files)} | - |\n"
                    total_files += len(files)
            
            output += f"\n**å°è®¡: {total_files} ä¸ªæ–‡ä»¶**\n\n"
        else:
            output += f"### ğŸ¬ {cls.name} (TMDB:{tmdb_id})\n\n"
            output += f"**æ–‡ä»¶æ•°**: {len(cls.files)} ä¸ª\n\n"
    
    # æœªåˆ†ç±»æ–‡ä»¶
    if summary["unmatched_files"]:
        output += f"## âš ï¸ æœªåŒ¹é…æ–‡ä»¶: {summary['unmatched']} ä¸ª\n\n"
        for r in summary["unmatched_files"][:10]:
            output += f"- {r.file_name}: {r.error_message}\n"
        if len(summary["unmatched_files"]) > 10:
            output += f"- ... è¿˜æœ‰ {len(summary['unmatched_files']) - 10} ä¸ª\n"
        output += "\n"
    
    # é”™è¯¯æ–‡ä»¶
    if summary["error_files"]:
        output += f"## âŒ é”™è¯¯æ–‡ä»¶: {summary['error']} ä¸ª\n\n"
        for r in summary["error_files"][:10]:
            output += f"- {r.file_name}: {r.error_message}\n"
        if len(summary["error_files"]) > 10:
            output += f"- ... è¿˜æœ‰ {len(summary['error_files']) - 10} ä¸ª\n"
        output += "\n"
    
    # ä¸‹ä¸€æ­¥æç¤º
    output += "---\n\n"
    output += "## ğŸ¯ ä¸‹ä¸€æ­¥\n\n"
    if summary["unmatched"] > 0 or summary["error"] > 0:
        output += "âš ï¸ æœ‰æœªåŒ¹é…/é”™è¯¯æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦ä¿®æ­£ mappings åé‡æ–°åˆ†ç±»ã€‚\n\n"
    output += "è¯·é€‰æ‹©æ“ä½œï¼š\n"
    output += "- **æ‰§è¡Œ STRM**: `connect_strm_target` â†’ `generate_strm`\n"
    output += "- **æ‰§è¡Œä¼ ç»Ÿæ•´ç†**: `organize_files`\n"
    output += "- **é‡æ–°åˆ†ç±»**: ä¿®æ­£ mappings åå†æ¬¡è°ƒç”¨ `analyze_and_classify_v2`\n"
    
    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    classifications_list = _classifications_to_list(classifications)
    
    # æ„å»ºå‰ç«¯æœŸæœ›çš„ classification_result æ ¼å¼ï¼ˆæŒ‰å­£æ˜¾ç¤ºï¼‰
    classification_result = {}
    for tmdb_id, cls in classifications.items():
        if cls.type == MediaType.TV:
            total_files = sum(len(files) for files in cls.seasons.values())
            
            # ğŸ”¥ æŒ‰å­£æ„å»ºè¯¦æƒ…
            seasons_info = []
            all_eps = []
            for season_num, files in sorted(cls.seasons.items()):
                eps = [f.episode for f in files if f.episode > 0]
                if eps:
                    all_eps.extend(eps)
                    season_ep_range = f"E{min(eps):02d}-E{max(eps):02d}"
                    seasons_info.append(SeasonInfo(
                        season=season_num,
                        episode_count=len(eps),
                        ep_range=season_ep_range
                    ))
            
            ep_range = f"E{min(all_eps):02d}-E{max(all_eps):02d}" if all_eps else "-"
            
            item = ClassificationResultItem(
                name=cls.name,
                file_count=total_files,
                ep_range=ep_range,
                type="tv",
                seasons=[s.model_dump() for s in seasons_info]  # ğŸ†• æŒ‰å­£è¯¦æƒ…
            )
            classification_result[str(tmdb_id)] = item.model_dump()
        else:
            item = ClassificationResultItem(
                name=cls.name,
                file_count=len(cls.files),
                ep_range="-",
                type="movie",
                seasons=[]
            )
            classification_result[str(tmdb_id)] = item.model_dump()
    
    return make_tool_response(
        output,
        state_update={
            "classifications": classifications_list,
            "classification_result": classification_result,
        }
    )


@tool
def get_status(
    state: Annotated[dict, InjectedState] = None,
) -> str:
    """
    è·å–å½“å‰çŠ¶æ€æ‘˜è¦
    
    Returns:
        ToolResponse JSONï¼ŒåŒ…å«å½“å‰çŠ¶æ€æ‘˜è¦
    """
    # ğŸ”¥ è°ƒè¯•æ—¥å¿—
    logger.info(f"ğŸ“Š get_status è¢«è°ƒç”¨")
    logger.info(f"ğŸ“Š state ç±»å‹: {type(state)}, æ˜¯å¦ä¸º None: {state is None}")
    if state:
        logger.info(f"ğŸ“Š state keys: {list(state.keys()) if hasattr(state, 'keys') else 'N/A'}")
        scanned_count = len(state.get("scanned_files", []))
        logger.info(f"ğŸ“Š scanned_files æ•°é‡: {scanned_count}")
    
    # ä» State è¯»å–æ•°æ®
    storage_config = state.get("storage_config", {}) if state else {}
    strm_target_config = state.get("strm_target_config", {}) if state else {}
    scanned_files_data = state.get("scanned_files", []) if state else []
    classifications_data = state.get("classifications", []) if state else []
    
    output = "## ğŸ“Š å½“å‰çŠ¶æ€\n\n"
    
    # è¿æ¥çŠ¶æ€
    if storage_config.get("url"):
        output += f"### æºå­˜å‚¨\n"
        output += f"- å·²è¿æ¥: {storage_config.get('url', '?')}\n"
        output += f"- åŸºç¡€è·¯å¾„: {storage_config.get('base_path', '/')}\n\n"
    else:
        output += "### æºå­˜å‚¨\n- âŒ æœªè¿æ¥\n\n"
    
    # STRM ç›®æ ‡
    if strm_target_config.get("connected"):
        output += f"### STRM ç›®æ ‡å­˜å‚¨\n"
        output += f"- å·²è¿æ¥: {strm_target_config.get('url', '?')}\n\n"
    
    # æ‰«æç»“æœ
    if scanned_files_data:
        videos = [f for f in scanned_files_data if f.get("type") == 'video']
        output += f"### æ‰«æç»“æœ\n"
        output += f"- è§†é¢‘æ–‡ä»¶: {len(videos)} ä¸ª\n\n"
    else:
        output += "### æ‰«æç»“æœ\n- æœªæ‰«æ\n\n"
    
    # åˆ†ç±»ç»“æœ
    if classifications_data:
        output += "### åˆ†ç±»ç»“æœ\n"
        for cls_dict in classifications_data:
            tmdb_id = cls_dict.get("tmdb_id")
            name = cls_dict.get("name", "Unknown")
            cls_type = cls_dict.get("type", "tv")
            
            if cls_type == "tv":
                total = sum(len(files) for files in cls_dict.get("seasons", {}).values())
            else:
                total = len(cls_dict.get("files", []))
            output += f"- {name} (TMDB:{tmdb_id}): {total} ä¸ªæ–‡ä»¶\n"
    else:
        output += "### åˆ†ç±»ç»“æœ\n- æœªåˆ†ç±»\n"
    
    return make_tool_response(output)


@tool
def list_files(
    filter_type: str = "all",
    limit: int = 50,
    offset: int = 0,
    pattern: str = "",
    state: Annotated[dict, InjectedState] = None,
) -> str:
    """
    ğŸ”§ è¾…åŠ©å·¥å…·ï¼šåˆ—å‡ºå·²æ‰«æçš„æ–‡ä»¶ï¼ˆä»…åœ¨éœ€è¦è°ƒè¯•æˆ–éªŒè¯æ—¶ä½¿ç”¨ï¼‰
    
    âš ï¸ ä½¿ç”¨åœºæ™¯ï¼ˆä»…é™ä»¥ä¸‹æƒ…å†µï¼‰ï¼š
    - ç”¨æˆ·æ˜ç¡®è¦æ±‚æŸ¥çœ‹å…·ä½“æ–‡ä»¶åˆ—è¡¨
    - åˆ†ç±»ç»“æœæœ‰ç–‘é—®ï¼Œéœ€è¦éªŒè¯åŸå§‹æ–‡ä»¶å
    - è°ƒè¯•æ—¶éœ€è¦ç¡®è®¤å“ªäº›æ–‡ä»¶æ²¡è¢«æ­£ç¡®åˆ†ç±»
    
    âŒ ä¸åº”ä½¿ç”¨ï¼ˆæ­£å¸¸æµç¨‹ä¸­ï¼‰ï¼š
    - å¸¸è§„æµç¨‹ï¼ˆscan â†’ analyze_and_classify â†’ generateï¼‰ä¸éœ€è¦æ­¤å·¥å…·
    - analyze_and_classify å·²è¿”å›ç›®å½•ç»“æ„å’Œæœªåˆ†ç±»æ–‡ä»¶
    
    Args:
        filter_type: ç­›é€‰ç±»å‹
            - "all": æ‰€æœ‰æ–‡ä»¶
            - "video": ä»…è§†é¢‘æ–‡ä»¶
            - "subtitle": ä»…å­—å¹•æ–‡ä»¶
            - "unclassified": ä»…æœªåˆ†ç±»æ–‡ä»¶ï¼ˆéœ€å…ˆæ‰§è¡Œ analyze_and_classifyï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤50ï¼Œæœ€å¤§200ï¼‰
        offset: åç§»é‡ï¼ˆåˆ†é¡µç”¨ï¼Œä»0å¼€å§‹ï¼‰
        pattern: æ–‡ä»¶ååŒ¹é…æ¨¡å¼ï¼ˆå¯é€‰ï¼Œå¦‚ "EP72" åŒ¹é…åŒ…å« EP72 çš„æ–‡ä»¶ï¼‰
    
    Returns:
        ToolResponse JSONï¼ŒåŒ…å«æ–‡ä»¶åˆ—è¡¨
    """
    # ğŸ”¥ è°ƒè¯•æ—¥å¿— - ä½¿ç”¨ print å’Œ logger ç¡®ä¿è¾“å‡º
    import sys
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ list_files è¢«è°ƒç”¨: filter_type={filter_type}, limit={limit}, offset={offset}, pattern={pattern}", file=sys.stderr)
    print(f"ğŸ”¥ğŸ”¥ğŸ”¥ state ç±»å‹: {type(state)}, æ˜¯å¦ä¸º None: {state is None}", file=sys.stderr)
    sys.stderr.flush()
    logger.warning(f"ğŸ“‹ list_files è¢«è°ƒç”¨: filter_type={filter_type}, limit={limit}, offset={offset}, pattern={pattern}")
    logger.warning(f"ğŸ“‹ state ç±»å‹: {type(state)}, æ˜¯å¦ä¸º None: {state is None}")
    if state:
        logger.info(f"ğŸ“‹ state keys: {list(state.keys()) if hasattr(state, 'keys') else 'N/A'}")
    
    # ä» State è¯»å–æ•°æ®
    scanned_files_data = state.get("scanned_files", []) if state else []
    classifications_data = state.get("classifications", []) if state else []
    
    logger.info(f"ğŸ“‹ scanned_files_data é•¿åº¦: {len(scanned_files_data)}")
    
    if not scanned_files_data:
        logger.warning("ğŸ“‹ scanned_files_data ä¸ºç©ºï¼Œè¿”å›é”™è¯¯")
        return make_tool_response("âŒ æœªæ‰«æï¼Œè¯·å…ˆä½¿ç”¨ scan_media_files æ‰«æç›®å½•")
    
    # è§£æ scanned_files
    scanned_files = _parse_scanned_files(scanned_files_data)
    
    # é™åˆ¶ limit æœ€å¤§å€¼
    limit = min(limit, 200)
    
    # ç­›é€‰æ–‡ä»¶
    files = scanned_files
    
    if filter_type == "video":
        files = [f for f in files if f.type == 'video']
    elif filter_type == "subtitle":
        files = [f for f in files if f.type == 'subtitle']
    elif filter_type == "unclassified":
        # è·å–å·²åˆ†ç±»çš„æ–‡ä»¶è·¯å¾„
        classified_paths = set()
        for cls_dict in classifications_data:
            cls_type = cls_dict.get("type", "tv")
            if cls_type == "movie":
                for cf in cls_dict.get("files", []):
                    classified_paths.add(cf.get("path"))
            else:
                for season_files in cls_dict.get("seasons", {}).values():
                    for cf in season_files:
                        classified_paths.add(cf.get("path"))
        files = [f for f in files if f.path not in classified_paths and f.type == 'video']
    
    # æŒ‰æ¨¡å¼ç­›é€‰
    if pattern:
        files = [f for f in files if pattern.lower() in f.name.lower()]
    
    # åˆ†é¡µ
    total = len(files)
    files = files[offset:offset + limit]
    
    if not files:
        return make_tool_response(f"ğŸ” æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼ˆç­›é€‰: {filter_type}, æ¨¡å¼: '{pattern}'ï¼‰")
    
    # æ„å»ºè¾“å‡º
    output = f"## ğŸ“‹ æ–‡ä»¶åˆ—è¡¨\n\n"
    output += f"- ç­›é€‰: `{filter_type}`"
    if pattern:
        output += f", æ¨¡å¼: `{pattern}`"
    output += f"\n- æ˜¾ç¤º: {offset + 1} - {offset + len(files)} / å…± {total} ä¸ª\n\n"
    
    output += "| åºå· | æ–‡ä»¶å | é›†æ•° | ç›®å½• |\n"
    output += "|-----|--------|-----|------|\n"
    
    for i, f in enumerate(files, start=offset + 1):
        name = f.name
        if len(name) > 40:
            name = name[:37] + '...'
        
        # æå–é›†æ•°
        episode = _extract_episode_number(f.name)
        ep_str = f"EP{episode:03d}" if episode else "-"
        
        # æå–ç›®å½•
        path = f.path
        parts = path.rsplit('/', 2)
        directory = parts[-2] if len(parts) >= 2 else '/'
        if len(directory) > 25:
            directory = directory[:22] + '...'
        
        output += f"| {i} | {name} | {ep_str} | {directory} |\n"
    
    if total > offset + limit:
        output += f"\nğŸ’¡ è¿˜æœ‰ {total - offset - limit} ä¸ªæ–‡ä»¶æœªæ˜¾ç¤ºï¼Œä½¿ç”¨ `offset={offset + limit}` æŸ¥çœ‹ä¸‹ä¸€é¡µ"
    
    return make_tool_response(output)
# Force reload Thu Jan  8 10:02:50 CST 2026
